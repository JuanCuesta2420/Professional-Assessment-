{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f424782",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdc48b4",
   "metadata": {},
   "source": [
    "#### Question 1: Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84f3d949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(4400)\n",
    "\n",
    "# Number of samples\n",
    "num_samples = 1000\n",
    "\n",
    "# Create 20 random variables\n",
    "X = pd.DataFrame()\n",
    "for i in np.arange(1, 21):\n",
    "    variable_name = f\"Var{i}\"\n",
    "    X[variable_name] = np.random.rand(1000)\n",
    "\n",
    "# Create a target variable based on some combination of the 20 variables\n",
    "y = (2 * X[\"Var1\"] + 0.5 * X[\"Var5\"] - 1 * X[\"Var10\"] + np.random.normal(0, 0.5, num_samples)  # Add some noise\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8cb450",
   "metadata": {},
   "source": [
    "1. Split the data into training and test data. The proportion of train data should be 70%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0e9a600b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=4400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dc7c18",
   "metadata": {},
   "source": [
    "2. Fit the model with a linear regression using all the features, report the coefficient table, intercept and MSE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "91f20065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE is 0.2833049093499735\n",
      "   Variable  Coefficient\n",
      "0     Var20     2.014610\n",
      "1     Var20    -0.081270\n",
      "2     Var20    -0.042229\n",
      "3     Var20     0.040105\n",
      "4     Var20     0.439223\n",
      "5     Var20     0.064899\n",
      "6     Var20     0.027779\n",
      "7     Var20     0.068921\n",
      "8     Var20    -0.073308\n",
      "9     Var20    -1.031674\n",
      "10    Var20    -0.001053\n",
      "11    Var20     0.052927\n",
      "12    Var20     0.009953\n",
      "13    Var20     0.024712\n",
      "14    Var20    -0.074581\n",
      "15    Var20    -0.072498\n",
      "16    Var20    -0.056306\n",
      "17    Var20     0.088936\n",
      "18    Var20     0.021261\n",
      "19    Var20     0.078586\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "y_linear = lm.predict(X_test)\n",
    "y_linear\n",
    "\n",
    "\n",
    "MSE = mean_squared_error(y_test, y_linear)\n",
    "print('The MSE is', MSE)\n",
    "\n",
    "\n",
    "lm.fit(X, y)\n",
    "coefficients = lm.coef_\n",
    "# Create a DataFrame to display the coefficients with variable names\n",
    "coef_df = pd.DataFrame({'Variable': variable_name, 'Coefficient': coefficients})\n",
    "# Print the DataFrame\n",
    "print(coef_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f3152f",
   "metadata": {},
   "source": [
    "3. Fit the model with a polynomial regression with degree 2, report the MSE. Is it necessary to use polynomial regression in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b087f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3912959808588364\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "degree = 2\n",
    "poly_features = PolynomialFeatures(degree=degree)\n",
    "X_poly = poly_features.fit_transform(X_train)\n",
    "\n",
    "\n",
    "# Create and fit the polynomial regression model\n",
    "poly_model = LinearRegression()\n",
    "poly_model.fit(X_poly, y_train)\n",
    "\n",
    "\n",
    "X_poly_test = poly_features.fit_transform(X_test)\n",
    "y_poly_test = poly_model.predict(X_poly_test)\n",
    "\n",
    "\n",
    "MSE = mean_squared_error(y_test, y_poly_test)\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47875873",
   "metadata": {},
   "source": [
    "It would not make any sense since we can see that the linear regression MSE is much lower than this MSE "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd6f08d",
   "metadata": {},
   "source": [
    "4. Fit the model with a Lasso regression, tune the parameter for the penalty parameter $\\alpha$. Report the best $\\alpha$, MSE and which variables are left in the model in the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec4dc1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "Mean Squared Error (MSE): 0.27893476514858456\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Lasso\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define a range of alpha values to tune\n",
    "alphas = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "\n",
    "# Create a parameter grid for GridSearchCV\n",
    "param_grid = {'alpha': alphas}\n",
    "\n",
    "\n",
    "lasso_model = Lasso()\n",
    "grid_search = GridSearchCV(lasso_model, param_grid, cv=10, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "# Get the best alpha value from the tuning\n",
    "best_alpha = grid_search.best_params_['alpha']\n",
    "print(best_alpha)\n",
    "\n",
    "\n",
    "# Fit a Ridge regression model with the best alpha value\n",
    "lasso_model = Lasso(alpha=best_alpha)\n",
    "lasso_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = lasso_model.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "# Calculate the Mean Squared Error (MSE) and R-squared (R2)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "\n",
    "# Print the results\n",
    "print(\"Mean Squared Error (MSE):\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e37ec2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Feature  Coefficient Estimate\n",
      "0     Var1              0.566406\n",
      "1     Var2             -0.012671\n",
      "2     Var3             -0.024085\n",
      "3     Var4              0.000000\n",
      "4     Var5              0.111930\n",
      "5     Var6              0.000000\n",
      "6     Var7              0.008001\n",
      "7     Var8              0.017024\n",
      "8     Var9             -0.013400\n",
      "9    Var10             -0.306822\n",
      "10   Var11              0.000000\n",
      "11   Var12              0.012300\n",
      "12   Var13              0.000000\n",
      "13   Var14             -0.000000\n",
      "14   Var15             -0.006134\n",
      "15   Var16             -0.002697\n",
      "16   Var17             -0.016334\n",
      "17   Var18              0.025483\n",
      "18   Var19              0.013672\n",
      "19   Var20              0.024080\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "\n",
    "coeff_df = DataFrame(X.columns)\n",
    "coeff_df.columns = [\"Feature\"]\n",
    "coeff_df[\"Coefficient Estimate\"] = pd.Series(lasso_model.coef_)\n",
    "print(coeff_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4386466",
   "metadata": {},
   "source": [
    "5. Define a new target variable $y_1$ such that $y_1$ only contains all the positive values in the $y$. Process $X$ as well. Fit the model with appropriate GLM model (not Gaussian). Report the MSE and can we compare the MSE with previous questions? Hint: it is a continous distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9b34fe20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for Gamma is:  0.26236130418363846\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from scipy.stats import gamma\n",
    "\n",
    "new_df = X.copy()\n",
    "new_df['y'] = y\n",
    "x_y = new_df[new_df['y'] > 0]\n",
    "y1 = x_y['y']\n",
    "new_df = x_y.drop(columns = [\"y\"])\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(new_df, y1, test_size = 0.3, \n",
    "                                                    random_state = 4400)\n",
    "\n",
    "gamma_model = sm.GLM(y_train, X_train, family = \n",
    "                     sm.families.Gamma(link = sm.families.links.log()))\n",
    "gamma_results = gamma_model.fit()\n",
    "y_pred_gamma = gamma_results.predict(X_test)\n",
    "mse_gamma = mean_squared_error(y_test, y_pred_gamma)\n",
    "\n",
    "print(\"MSE for Gamma is: \", mse_gamma)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefb1be7",
   "metadata": {},
   "source": [
    "6. Define a new target variable $y_2$ such that $y_2$ is a binary categorical variable. If $y$ is larger than 1, then $y_2$ is \"group1\", otherwise it is \"group2\". Fit the $y_2$ and $X$ with a logistic regression. Print the summary table with .summary(), and interpret the coefficient for variable 1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5762ddc0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'method' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-645b4ea0881c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx_y2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx_y2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y2'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mx_y2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_y2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_y2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y2'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_y2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'method' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "x_y2 = X.copy \n",
    "x_y2['y2'] = y.apply (lambda x: 0 if x > 1 else 1)\n",
    "x_y2 = x_y2[x_y2['y2'] == 0]\n",
    "\n",
    "y2 = x_y2['y2']\n",
    "x_y2 = x_y2.drop(columns = [\"y2\"])\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(new_df, y1, test_size = 0.3, \n",
    "                                                    random_state = 4400)\n",
    "\n",
    "model = sm.GLM(y_train, X_train, family = sm.families.Binomial())\n",
    "results = model.fit()\n",
    "y_pred = results.predict(X_test)\n",
    "MSE = mean_squared_error(y_test, y_pred)\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1a7ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new dataframe with a binary column (instead of group 1 and 2) where group 1 is 0 and group2 is 1\n",
    "X_Y2 = X.copy()\n",
    "X_Y2['y2'] = y.apply(lambda x: 0 if x > 1 else 1)\n",
    "\n",
    "\n",
    "#We want to isolate the y2 (so when y is larger than 0, so the binary number 0)\n",
    "X_Y2 = X_Y2[X_Y2['y2'] == 0]\n",
    "\n",
    "#Get all the group 1 values\n",
    "y2 = X_Y2['y2']\n",
    "\n",
    "#Drop the Y column\n",
    "X_Y2 = X_Y2.drop(columns = [\"y2\"])\n",
    "\n",
    "\n",
    "X = sm.add_constant(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60df7b8",
   "metadata": {},
   "source": [
    "#### Question 2: Implement Gradient descend for Polynomial Regression\n",
    "\n",
    "Implement gradient descend method for the polynomial regression. Requirement: \n",
    "1. Write the method as a function, which is given here. Notice that it takes an input \"degree\" (and any other necessary inputs) so that we can change the degree of the polynomial. \n",
    "2. Output the cost history as well as the coefficient estimates. No need to print it or make the figures. As long as it is one of the output. \n",
    "3. Verify your function with the data in Question 1 (You may need to copy/paste and run the answer in Question 1-1 before you run the verification). No need to compare your coefficients to the ones in question 1. This step is only to make sure your functions work. You can set the degree as 2 in the verification. \n",
    "\n",
    "Hint: \n",
    "1. Don't overthinking the question. What is the difference between linear regression and polynomial regression?\n",
    "2. You may need two functions here. One for pre-processing the data, while the other one for gradient desent. You can add more if you need. Like to add another one for the cost function. \n",
    "3. When initializing the theta, think about how many coefficients you may need?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "17f75177",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def poly_function(degree, X):\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    return X_poly\n",
    "\n",
    "def compute_cost(X, y, theta):\n",
    "    m = len(y)\n",
    "    predictions = np.dot(X, theta)\n",
    "    cost = (1 / (2 * m)) * np.sum(np.square(predictions - y))\n",
    "    return cost\n",
    "\n",
    "def gradient_descent(X, y, num_iterations, learning_rate, degree):\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros(n)\n",
    "    cost_history = np.zeros(num_iterations)\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        predictions = np.dot(X, theta)\n",
    "        errors = predictions - y\n",
    "        gradient = (1 / m) * np.dot(X.T, errors)\n",
    "        theta -= learning_rate * gradient\n",
    "        cost_history[i] = compute_cost(X, y, theta)\n",
    "\n",
    "    return theta, cost_history\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8c5a77e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(4400)\n",
    "\n",
    "# Number of samples\n",
    "num_samples = 1000\n",
    "\n",
    "# Create 20 random variables\n",
    "X = pd.DataFrame()\n",
    "for i in np.arange(1, 21):\n",
    "    variable_name = f\"Var{i}\"\n",
    "    X[variable_name] = np.random.rand(1000)\n",
    "\n",
    "# Create a target variable based on some combination of the 20 variables\n",
    "y = (2 * X[\"Var1\"] + 0.5 * X[\"Var5\"] - 1 * X[\"Var10\"] + np.random.normal(0, 0.5, num_samples)  # Add some noise\n",
    "    )\n",
    "\n",
    "degree = 2\n",
    "X_poly = poly_function(degree, X)\n",
    "theta, cost_history = gradient_descent(X_poly, y, num_iterations=1000, learning_rate=0.01, degree=degree)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d09d29",
   "metadata": {},
   "source": [
    "#### Question 3: Simulation study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7d7428",
   "metadata": {},
   "source": [
    "Following is a simulation study. In the second code chuck, please correctly label the xlabel and ylabel for the plot. Also explain what this code is trying to do and what you have learned from the generated figure. \n",
    "\n",
    "Hint: in the simulation data, there are 200 observations and it is fixed for each trial. After spliting the training and testing data, each one will have 100 observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b6315271",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(83)\n",
    "A = np.arange(5,101,1)\n",
    "B = []\n",
    "\n",
    "for p in A:\n",
    "    \n",
    "    X = pd.DataFrame()\n",
    "    for i in np.arange(1, p+1):\n",
    "        variable_name = f\"Var{i}\"\n",
    "        X[variable_name] = np.random.rand(200)\n",
    "\n",
    "    y = 2 * X[\"Var1\"] - 0.5 * X[\"Var5\"] + np.random.normal(0, 0.5, 200)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                        test_size=0.5, \n",
    "                                                        random_state=83)\n",
    "    \n",
    "    lm = LinearRegression()\n",
    "    lm.fit(X_train, y_train)\n",
    "    y_pred = lm.predict(X_test)\n",
    "    value = mean_squared_error(y_test, y_pred)\n",
    "    B.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "20689577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(A, B)\n",
    "plt.xlabel(\"Number of variables (p)\")\n",
    "plt.ylabel(\"Mean squared errors (MSE)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66846ef",
   "metadata": {},
   "source": [
    "In this graph we can see that MSE is calculated and it is added to a new list called B. We can see that the MSE is calculated for each of the variable and then added to it. So we can say that Column B is a list of Mean Squared Errors and lis A is a random variable from 5 to 100. What we can see in this graph is a line graph that shows how the MSE icreases as more variables are added to the dataset. It makes us understand that adding more variables to the model affects its predictive accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514f75a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
